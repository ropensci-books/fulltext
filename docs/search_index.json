[["index.html", "fulltext manual Chapter 1 fulltext manual 1.1 Info 1.2 Citing fulltext 1.3 Installation", " fulltext manual built on 2021-10-04 - for fulltext v2.0 Chapter 1 fulltext manual An R package to search across and get full text for journal articles The fulltext package makes it easy to do text-mining by supporting the following steps: Search for articles Fetch articles Get links for full text articles (xml, pdf) Extract text from articles / convert formats Collect bits of articles that you actually need Download supplementary materials from papers 1.1 Info Code: https://github.com/ropensci/fulltext/ Package Documentation: https://docs.ropensci.org/fulltext Issues/Bug reports: https://github.com/ropensci/fulltext/issues CRAN: https://cran.rstudio.com/web/packages/fulltext/ 1.2 Citing fulltext Scott Chamberlain (2019). fulltext: Full Text of ‘Scholarly’ Articles Across Many Data Sources. R package version 2.0. https://docs.ropensci.org/fulltext (website) https://github.com/ropensci/fulltext/ (devel) https://books.ropensci.org/fulltext/ (user manual) 1.3 Installation Stable version from CRAN install.packages(&quot;fulltext&quot;) Development version from GitHub remotes::install_github(&quot;ropensci/fulltext&quot;) Load library library(&#39;fulltext&#39;) "],["intro.html", "Chapter 2 Introduction 2.1 User interface", " Chapter 2 Introduction 2.1 User interface Functions in fulltext are setup to make the package as easy to use as possible. The functions are organized around use cases: Search for articles Get full text links Get articles Get abstracts Pull out article sections of interest Because there are so many data sources for scholarly texts, it makes a lot of sense to simplify the details of each data source, and present a single user interface to all of them. "],["data-sources.html", "Chapter 3 Data sources 3.1 Search 3.2 Abstracts 3.3 Links 3.4 Getting full text", " Chapter 3 Data sources Data sources in fulltext include: Crossref - via the rcrossref package Public Library of Science (PLOS) - via the rplos package Biomed Central arXiv - via the aRxiv package bioRxiv - via the biorxivr package PMC/Pubmed via Entrez - via the rentrez package Many more are supported via the above sources (e.g., Royal Society Open Science is available via Pubmed) We will add more, as publishers open up, and as we have time…See the master list here Data sources will differ by the task you are doing in fulltext. 3.1 Search When searching with ft_search() you’ll have access to a specific set of sources and no others, including: arxiv biorxivr bmc crossref entrez europe_pmc ma plos scopus You can see what plugins there are with ft_search_ls() 3.2 Abstracts When using ft_abstract() you have access to: crossref microsoft plos scopus semanticscholar You can see what plugins there are with ft_abstract_ls() 3.3 Links When using ft_links() to get links to full text, you’ll have access to: bmc cdc cogent copernicus crossref elife entrez frontiersin peerj plos rsoc You can see what plugins there are with ft_links_ls() 3.4 Getting full text While using ft_get() to fetch full text of articles you’ll have access to a set of specific data sources (in this case publishers) for which we have some coded plugins (i.e., functions): aaas aip amersocclinoncol amersocmicrobiol arxiv biorxiv bmc cambridge cob copernicus crossref elife elsevier entrez frontiersin ieee informa instinvestfil jama microbiology peerj pensoft plos pnas royalsocchem roysoc sciencedirect scientificsocieties transtech wiley You can see what plugins there are with ft_get_ls() But there are also other options within ft_get() that we take advantage of. This is because DOIs (Digital Object Identifiers) which you feed into ft_get() have a prefix that is affiliated with a specific publisher. We can then decide whether to use one of our plugins listed in ft_get_ls() or something else. If we don’t have a plugin we first look to see if Crossref has the full text link to either XML or PDF for the DOI. If not, we then go to an API rOpenSci maintains. This API has a set of rules for each publisher - some of which are simple rules like add a URL plus a DOI - but some require an HTTP request then some string manipulation. "],["authentication.html", "Chapter 4 Authentication", " Chapter 4 Authentication Some data sources require authentication. Here’s a breakdown of how to do authentication by data source: BMC: BMC is integrated into Springer Publishers now, and that API requires an API key. Get your key by signing up at https://dev.springer.com/, then you’ll get a key. Pass the key to a named parameter key to bmcopts. Or, save your key in your .Renviron file as SPRINGER_KEY, and we’ll read it in for you, and you don’t have to pass in anything. Scopus: Scopus requires two things: an API key and your institution must have access. For the API key, go to https://dev.elsevier.com/index.html, register for an account, then when you’re in your account, create an API key. Pass in as variable key to scopusopts, or store your key under the name ELSEVIER_SCOPUS_KEY as an environment variable in .Renviron, and we’ll read it in for you. See Startup for help. For the institution access go to a browser and see if you have access to the journal(s) you want. If you don’t have access in a browser you probably won’t have access via this package. If you aren’t physically at your institution you will likely need to be on a VPN or similar and eventually require correct proxy settings, so that your IP address is in the range that the two publishers are accepting for that institution. It might be, that the API access seems to work even while in the wrong IP range or have wrong proxy settings, but you are not able to see the abstracts, they will be empty. By using the currect curl options into the calls to ft_search or ft_abstracts even the most complex proxy including authentication should work. As an example: ft_abstract(x = dois, from = &quot;scopus&quot;, scopusopts = opts, proxy=&quot;proxy-ip-address&quot;, proxyport=your-proxy-port, proxyuserpwd=&quot;username:password&quot;, # often the same as your windows login proxyauth=8) # ntlm - authentication ScienceDirect: Elsevier ScienceDirect requires two things: an API key and your institution must have access. For the API key, go to https://dev.elsevier.com/index.html, register for an account, then when you’re in your account, create an API key that is allowed to access the TDM API (must accept their TDM policy). Pass in as variable key to sciencedirectopts, or store your key under the name ELSEVIER_TDM_KEY as an environment variable in .Renviron, and we’ll read it in for you. See Startup for help. For the institution access go to a browser and see if you have access to the journal(s) you want. If you don’t have access in a browser you probably won’t have access via this package. If you aren’t physically at your institution you will likely need to be on a VPN or similar so that your IP address is in the range that the publisher is accepting for that institution. Microsoft: Get a key by creating an Azure account at https://www.microsoft.com/cognitive-services/en-us/subscriptions, then requesting a key for Academic Knowledge API within Cognitive Services. Store it as an environment variable in your .Renviron file - see Startup for help. Pass your API key into maopts as a named element in a list like list(key = Sys.getenv('MICROSOFT_ACADEMIC_KEY')) Crossref: Crossref encourages requests with contact information (an email address) and will forward you to a dedicated API cluster for improved performance when you share your email address with them. https://github.com/CrossRef/rest-api-doc#good-manners--more-reliable-service To pass your email address to Crossref via this client, store it as an environment variable in .Renviron like crossref_email = name@example.com Crossref TDM: TDM = “Text and Data Mining”. This applies to the few publishers - Wiley and Elsevier - that are part of this program (TDM). When using ft_get(), and you want to get papers from these two publishers, you’ll need two things: (1) an API key for the Crossref TDM. Go to https://apps.crossref.org/clickthrough/researchers and you’ll be asked to login with your ORCID. If you don’t have an ORCID go to https://orcid.org/ and get one. After logging in with your ORCID, click on the “API token” tag and grab your API key. Put your API key in .Renviron file or similar (e.g. .zshrc or .bash_profile, etc.) with the entry CROSSREF_TDM=yourkey. We’ll look for the environment variable CROSSREF_TDM within this package. See http://tdmsupport.crossref.org/ for more information on the Crossref TDM program. (2) Your institution needs to have access to the journal you’re requesting papers from. If you’re not sure about this just go to a browser and see if you have access to the journal(s) you want. If you don’t have access in a browser you probably won’t have access via this package. If you aren’t physically at your institution you will likely need to be on a VPN or similar so that your IP address is in the range that the two publishers are accepting for that institution. Also talk to your librarian if you aren’t sure about access or have questions about it. In some cases, you may also need to request that Elsevier removes a “fence” for your institution - that is, your institution has access to XYZ journal(s), but they don’t yet allow programmatic access. This has happened at least a few times that I know of. Entrez: NCBI limits users to making only 3 requests per second. But, users who register for an API key are able to make up to ten requests per second. Getting a key is simple; register for a “my ncbi” account then click on a button in the account settings page. Once you have an API key, you can pass it as the argument api_key to entrezopts in both ft_get() and ft_search(). However, we advise you use environment variables instead as they are more secure. To do that you can set an environment variable for the current R session like Sys.setenv(ENTREZ_KEY=\"yourkey\") OR better yet set it in your .Renviron or equivalent file with an entry like ENTREZ_KEY=yourkey so that it is used across R sessions. No authentication needed for PLOS, eLife, arxiv, biorxiv, Euro PMC "],["rate-limits.html", "Chapter 5 Rate limits", " Chapter 5 Rate limits Scopus: 20,000 per 7 days. See https://dev.elsevier.com/api_key_settings.html for rate limit information. To see what your personal rate limit details are, request verbose HTTP request output - this will vary on the function you are using - see the docs for the function. See the response headers X-RateLimit-Limit, X-RateLimit-Remaining, and X-RateLimit-Reset (your limit, those requests remaining, and UTC date/time it will reset) Microsoft: 10,000 per month, and 1 per second. There are no rate limit headers, sorry :( PLOS: There are no known rate limits for PLOS, though if you do hit something let us know. Crossref: From time to time Crossref needs to impose rate limits to ensure that the free API is usable by all. Any rate limits that are in effect will be advertised in the X-Rate-Limit-Limit and X-Rate-Limit-Interval HTTP headers. This boils down to: they allow X number of requests per some time period. The numbers can change so we can’t give a rate limit that will always be in effect. If you’re curious pass in verbose = TRUE to your function call, and you’ll get headers that will display these rate limits. See also Authentication. Semantic Scholar: Not documented in their docs, and no response headers given. At time of this writing (2020-07-01) the rate limit is: 100 requests per 5-minutes per IP address. or 20 requests per min. Note that this rate limit may change. "],["search.html", "Chapter 6 Search 6.1 Usage 6.2 Search many sources 6.3 Search options", " Chapter 6 Search Search is what you’ll likely start with for a number of reasons. First, search functionality in fulltext means that you can start from searching on words like ‘ecology’ or ‘cellular’ - and the output of that search can be fed downstream to the next major task: fetching articles. 6.1 Usage library(fulltext) List backends available ft_search_ls() #&gt; [1] &quot;arxiv&quot; &quot;biorxivr&quot; &quot;bmc&quot; &quot;crossref&quot; &quot;entrez&quot; #&gt; [6] &quot;europe_pmc&quot; &quot;ma&quot; &quot;plos&quot; &quot;scopus&quot; Search - by default searches against PLOS (Public Library of Science) res &lt;- ft_search(query = &quot;ecology&quot;) The output of ft_search is a ft S3 object, with a summary of the results: res #&gt; Query: #&gt; [ecology] #&gt; Found: #&gt; [PLoS: 58840; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] #&gt; Returned: #&gt; [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] and has slots for each data source: names(res) #&gt; [1] &quot;plos&quot; &quot;bmc&quot; &quot;crossref&quot; &quot;entrez&quot; &quot;arxiv&quot; &quot;biorxiv&quot; &quot;europmc&quot; #&gt; [8] &quot;scopus&quot; &quot;ma&quot; Get data for a single source res$plos #&gt; Query: [ecology] #&gt; Records found, returned: [58840, 10] #&gt; License: [CC-BY] #&gt; # A tibble: 10 × 1 #&gt; id #&gt; &lt;chr&gt; #&gt; 1 10.1371/journal.pone.0001248 #&gt; 2 10.1371/journal.pone.0248090 #&gt; 3 10.1371/journal.pone.0059813 #&gt; 4 10.1371/journal.pone.0080763 #&gt; 5 10.1371/journal.pone.0246749 #&gt; 6 10.1371/journal.pone.0254411 #&gt; 7 10.1371/journal.pone.0220747 #&gt; 8 10.1371/journal.pone.0155019 #&gt; 9 10.1371/journal.pone.0175014 #&gt; 10 10.1371/journal.pone.0241618 Note how in the metadata section above the data.frame of results we clearly state the license for articles for the given data source. For some data sources, licenses are the same for each paper; sometimes they vary among papers. 6.2 Search many sources Here, search for the term “ecology” across PLOS, Crossref, and arXiv preprint server. res &lt;- ft_search(query=&#39;ecology&#39;, from=c(&#39;plos&#39;,&#39;crossref&#39;,&#39;arxiv&#39;)) res #&gt; Query: #&gt; [ecology] #&gt; Found: #&gt; [PLoS: 58840; BMC: 0; Crossref: 229003; Entrez: 0; arxiv: 2629; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] #&gt; Returned: #&gt; [PLoS: 10; BMC: 0; Crossref: 10; Entrez: 0; arxiv: 10; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] Each source may have different results AND with different columns in each data.frame res$plos #&gt; Query: [ecology] #&gt; Records found, returned: [58840, 10] #&gt; License: [CC-BY] #&gt; # A tibble: 10 × 1 #&gt; id #&gt; &lt;chr&gt; #&gt; 1 10.1371/journal.pone.0001248 #&gt; 2 10.1371/journal.pone.0248090 #&gt; 3 10.1371/journal.pone.0059813 #&gt; 4 10.1371/journal.pone.0080763 #&gt; 5 10.1371/journal.pone.0246749 #&gt; 6 10.1371/journal.pone.0254411 #&gt; 7 10.1371/journal.pone.0220747 #&gt; 8 10.1371/journal.pone.0155019 #&gt; 9 10.1371/journal.pone.0175014 #&gt; 10 10.1371/journal.pone.0241618 res$arxiv #&gt; Query: [ecology] #&gt; Records found, returned: [2629, 10] #&gt; License: [variable, but should be free to text-mine, see http://arxiv.org/help/license and http://arxiv.org/help/bulk_data] #&gt; # A tibble: 10 × 15 #&gt; id submitted updated title abstract authors affiliations link_abstract #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 hep-p… 1993-03-0… 1993-03… &quot;Coul… &quot; We e… John E… &quot;&quot; http://arxiv… #&gt; 2 cond-… 1993-09-2… 1994-01… &quot;The … &quot; Natu… Dallas… &quot;&quot; http://arxiv… #&gt; 3 chao-… 1993-11-2… 1993-11… &quot;Rele… &quot; Netw… Kunihi… &quot;University… http://arxiv… #&gt; 4 adap-… 1993-11-2… 1993-11… &quot;Chao… &quot; The … Kunihi… &quot;University… http://arxiv… #&gt; 5 chao-… 1994-08-1… 1994-08… &quot;Weak… &quot; Popu… Shin-i… &quot;Department… http://arxiv… #&gt; 6 adap-… 1994-12-2… 1994-12… &quot;Pref… &quot; Part… Dan As… &quot;Dept. of M… http://arxiv… #&gt; 7 comp-… 1995-06-0… 1995-06… &quot;Pred… &quot; In p… H. P. … &quot;Institute … http://arxiv… #&gt; 8 math/… 1995-07-0… 1995-07… &quot;Nece… &quot; This… Robert… &quot;&quot; http://arxiv… #&gt; 9 cond-… 1996-02-0… 1996-02… &quot;Mass… &quot; It i… Per Ba… &quot;&quot; http://arxiv… #&gt; 10 cond-… 1996-07-0… 1998-09… &quot;Simp… &quot; A ma… Susann… &quot;&quot; http://arxiv… #&gt; # … with 7 more variables: link_pdf &lt;chr&gt;, link_doi &lt;chr&gt;, comment &lt;chr&gt;, #&gt; # journal_ref &lt;chr&gt;, doi &lt;chr&gt;, primary_category &lt;chr&gt;, categories &lt;chr&gt; res$crossref #&gt; Query: [ecology] #&gt; Records found, returned: [229003, 10] #&gt; License: [variable, see individual records] #&gt; # A tibble: 10 × 32 #&gt; container.title created deposited published.print doi indexed issn issue #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Functional Ecol… 2009-01… 2021-07-… 2009-02 10.1… 2021-0… 0269… 1 #&gt; 2 Ecology 2006-05… 2018-08-… 1957-10 10.2… 2021-0… 0012… 4 #&gt; 3 Ecology 2006-05… 2018-08-… 1957-07 10.2… 2021-0… 0012… 3 #&gt; 4 Journal of Indu… 2014-11… 2021-07-… 2014-12 10.1… 2021-0… 1088… 6 #&gt; 5 Ecology 2006-05… 2018-08-… 1942-04 10.2… 2021-0… 0012… 2 #&gt; 6 Ecology 2017-04… 2021-07-… 2017-06 10.1… 2021-0… 0012… 6 #&gt; 7 Ecology 2006-05… 2018-08-… 1966-09 10.2… 2021-0… 0012… 5 #&gt; 8 Ecology 2006-05… 2018-08-… 1927-10 10.2… 2021-0… 0012… 4 #&gt; 9 Ecology 2006-05… 2018-08-… 1976-11 10.2… 2021-0… 0012… 6 #&gt; 10 Ecology 2006-05… 2018-08-… 1927-04 10.2… 2021-0… 0012… 2 #&gt; # … with 24 more variables: issued &lt;chr&gt;, member &lt;chr&gt;, page &lt;chr&gt;, #&gt; # prefix &lt;chr&gt;, publisher &lt;chr&gt;, score &lt;chr&gt;, source &lt;chr&gt;, #&gt; # reference.count &lt;chr&gt;, references.count &lt;chr&gt;, #&gt; # is.referenced.by.count &lt;chr&gt;, subject &lt;chr&gt;, title &lt;chr&gt;, type &lt;chr&gt;, #&gt; # url &lt;chr&gt;, volume &lt;chr&gt;, language &lt;chr&gt;, author &lt;list&gt;, link &lt;list&gt;, #&gt; # license &lt;list&gt;, reference &lt;list&gt;, short.container.title &lt;chr&gt;, #&gt; # archive &lt;chr&gt;, published.online &lt;chr&gt;, subtitle &lt;chr&gt; Note above how licenses for PLOS are all CC-BY, whereas licenses in arXiv and Crossref are variable. For arXiv we don’t get any license information in the results. But for Crossref we do get license information. Let’s get license information for the first article: res$crossref$data$license[[1]] #&gt; # A tibble: 1 × 4 #&gt; date content.version delay.in.days URL #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 2015-09-01 tdm 2403 http://doi.wiley.com/10.1002/tdm_lic… It shows a license specific to Wiley, and gives the URL so you can look it up. 6.3 Search options Each of the data sources in ft_search can accept additional configuration. See the ?ft_search docs for details. Each data source has a parameter in ft_search, e.g, europmc data source can be configured with the euroopts parameter. Each of the *opts parameters expects a named list. Here, we search the phrase “ecology” at Europe PMC. res &lt;- ft_search(query=&#39;ecology&#39;, from=&#39;europmc&#39;) res$europmc #&gt; Query: [ecology] #&gt; Records found, returned: [425357, 10] #&gt; #&gt; # A tibble: 10 × 27 #&gt; id source pmid doi title authorString journalTitle journalVolume #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 34563792 MED 34563792 10.1… &quot;Fre… Yin B, Li J… J Plant Phy… 266 #&gt; 2 34593996 MED 34593996 10.1… &quot;Mic… Mason-Jones… ISME J &lt;NA&gt; #&gt; 3 34579579 MED 34579579 10.1… &quot;Mic… Frederickso… mBio &lt;NA&gt; #&gt; 4 34583083 MED 34583083 10.1… &quot;Cor… Yin X, Zhan… Sci Total E… 806 #&gt; 5 34597569 MED 34597569 10.1… &quot;New… Stockmann M… Sci Total E… &lt;NA&gt; #&gt; 6 34129698 MED 34129698 10.1… &quot;Low… Kreider MR,… Ecology 102 #&gt; 7 34498255 MED 34498255 10.1… &quot;Err… &lt;NA&gt; Ecology 102 #&gt; 8 34600006 MED 34600006 10.1… &quot;Eff… Deng J, Zho… Chemosphere &lt;NA&gt; #&gt; 9 34523534 MED 34523534 10.1… &quot;Bis… Zhou Y, Guo… Environ Pol… 290 #&gt; 10 IND607357494 AGR &lt;NA&gt; 10.1… &quot;Soi… Vieira AF, … Appl Soil E… 167 #&gt; # … with 19 more variables: pubYear &lt;chr&gt;, journalIssn &lt;chr&gt;, pageInfo &lt;chr&gt;, #&gt; # pubType &lt;chr&gt;, isOpenAccess &lt;chr&gt;, inEPMC &lt;chr&gt;, inPMC &lt;chr&gt;, hasPDF &lt;chr&gt;, #&gt; # hasBook &lt;chr&gt;, hasSuppl &lt;chr&gt;, citedByCount &lt;int&gt;, hasReferences &lt;chr&gt;, #&gt; # hasTextMinedTerms &lt;chr&gt;, hasDbCrossReferences &lt;chr&gt;, hasLabsLinks &lt;chr&gt;, #&gt; # hasTMAccessionNumbers &lt;chr&gt;, firstIndexDate &lt;chr&gt;, #&gt; # firstPublicationDate &lt;chr&gt;, issue &lt;chr&gt; Then get the next batch of results, using the cursorMark result ft_search(query=&#39;ecology&#39;, from=&#39;europmc&#39;, euroopts = list(cursorMark = res$europmc$cursorMark)) #&gt; Query: #&gt; [ecology] #&gt; Found: #&gt; [PLoS: 0; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 425357; Scopus: 0; Microsoft: 0] #&gt; Returned: #&gt; [PLoS: 0; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 10; Scopus: 0; Microsoft: 0] "],["abstracts.html", "Chapter 7 Abstracts 7.1 Usage 7.2 By Ids 7.3 Abstracts options", " Chapter 7 Abstracts Fetching abstracts likely will come after searching for articles with ft_search(). There are a few scenarios in which simply getting abstracts in lieu of full text may be enough. For example, if you know that a large portion of the articles you want to mine text from are closed access and you don’t have access to them, you may have access to the abstracts depending on the publisher. In addition, there are cases in which you really only need abstracts regardless of whether full text is available or not. ft_abstract() gives you access to the following data sources: crossref microsoft plos scopus semanticscholar 7.1 Usage library(fulltext) List data sources available ft_abstract_ls() #&gt; [1] &quot;crossref&quot; &quot;microsoft&quot; &quot;plos&quot; &quot;scopus&quot; #&gt; [5] &quot;semanticscholar&quot; Search - by default searches against PLOS (Public Library of Science) res &lt;- ft_search(query = &quot;ecology&quot;) (dois &lt;- res$plos$data$id) #&gt; [1] &quot;10.1371/journal.pone.0001248&quot; &quot;10.1371/journal.pone.0248090&quot; #&gt; [3] &quot;10.1371/journal.pone.0059813&quot; &quot;10.1371/journal.pone.0080763&quot; #&gt; [5] &quot;10.1371/journal.pone.0246749&quot; &quot;10.1371/journal.pone.0254411&quot; #&gt; [7] &quot;10.1371/journal.pone.0220747&quot; &quot;10.1371/journal.pone.0155019&quot; #&gt; [9] &quot;10.1371/journal.pone.0175014&quot; &quot;10.1371/journal.pone.0241618&quot; Take the output of ft_search() and pass directly to ft_abstract(): out &lt;- ft_abstract(dois) out #&gt; &lt;fulltext abstracts&gt; #&gt; Found: #&gt; [Crossref: 4; Scopus: 0; Microsoft: 0; PLOS: 0; Semantic Scholar: 0] The output has slots for each data source: names(out) #&gt; [1] &quot;crossref&quot; &quot;plos&quot; &quot;scopus&quot; &quot;ma&quot; #&gt; [5] &quot;semanticscholar&quot; Index to the data source you want to get data from, here selecting the first item: out$plos[[1]] #&gt; $doi #&gt; [1] &quot;10.1371/journal.pone.0001248&quot; #&gt; #&gt; $abstract #&gt; [1] &quot;&quot; Which gives a named list, with the DOI as the first element, then the abstract as a single character string. You can then take these abstracts and use any number of R packages for text mining. 7.2 By Ids Instead of using ft_search() first, and passing those results to ft_abstract(), you can pass article ids (character/numeric) to ft_abstract(). Here, we’ll fetch abstracts for three articles from arXiv. With Semantic Scholar we need to prefix the string arXiv to the ids (if you use DOIs you don’t need to prefix any string). arxiv_ids &lt;- c(&quot;0710.3491&quot;, &quot;0804.0713&quot;, &quot;0810.4821&quot;, &quot;1003.0315&quot;) out &lt;- ft_abstract(x = paste0(&quot;arXiv:&quot;, arxiv_ids), from = &quot;semanticscholar&quot;) unname(vapply(out$semanticscholar, &quot;[[&quot;, &quot;&quot;, &quot;abstract&quot;)) 7.3 Abstracts options All data sources for ft_abstract() accept configuration options as a named list. For example, if you set from=\"plos\" you can set additional PLOS specfiic options by passing a named list to plosopts. You can find out what PLOS options are available by looking at the documention for ?rplos::searchplos. The only data source that doesn’t allow configuration options is Semantic Scholar. As all functions in fulltext, you can pass on curl options to each function call or set them globally for the session, see the curl options chapter. "],["links.html", "Chapter 8 Links 8.1 Usage 8.2 Links options", " Chapter 8 Links The ft_links function makes it easy to get URLs for full text versions of articles. You can for instance only use fulltext to pass DOIs directly to ft_links to get URLs to use elsewhere in your research workflow. Or you may want to search first with ft_search, then pass that output directly to ft_links. 8.1 Usage library(fulltext) List backends available ft_links_ls() #&gt; [1] &quot;bmc&quot; &quot;cdc&quot; &quot;cogent&quot; &quot;copernicus&quot; &quot;crossref&quot; #&gt; [6] &quot;elife&quot; &quot;entrez&quot; &quot;frontiersin&quot; &quot;peerj&quot; &quot;plos&quot; #&gt; [11] &quot;rsoc&quot; You can pass DOIs directly to ft_links res &lt;- ft_links(&#39;10.3389/fphar.2014.00109&#39;) res #&gt; &lt;fulltext links&gt; #&gt; [Found] 1 #&gt; [IDs] 10.3389/fphar.2014.00109 ... The output is an S3 object, essentially a list. If you don’t specify a from value, we try to guess the publisher and the named list in the output will match the publisher of the DOI. Here, that’s Frontiers In, lowercased and as one word: res$frontiersin #&gt; $found #&gt; [1] 1 #&gt; #&gt; $ids #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; #&gt; #&gt; $data #&gt; $data$`10.3389/fphar.2014.00109` #&gt; $data$`10.3389/fphar.2014.00109`$xml #&gt; [1] &quot;http://journal.frontiersin.org/article/10.3389/fphar.2014.00109/xml/nlm&quot; #&gt; #&gt; $data$`10.3389/fphar.2014.00109`$pdf #&gt; [1] &quot;http://journal.frontiersin.org/article/10.3389/fphar.2014.00109/pdf&quot; The output is a named list with number of links found, the id (aka DOI), and in the $data slot is the links, which can include links for pdf, xml, plain (for plain text), unspecified and possibly others (publishers do lots of weird things). Instead of passing DOIs directly, you can use ft_search(() to search first: (res1 &lt;- ft_search(query=&#39;ecology&#39;, from=&#39;entrez&#39;)) #&gt; Query: #&gt; [ecology] #&gt; Found: #&gt; [PLoS: 0; BMC: 0; Crossref: 0; Entrez: 248883; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] #&gt; Returned: #&gt; [PLoS: 0; BMC: 0; Crossref: 0; Entrez: 10; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] Then pass the output of that directly to ft_links (out &lt;- ft_links(res1)) #&gt; &lt;fulltext links&gt; #&gt; [Found] 5 #&gt; [IDs] ID_34597326 ID_34597314 ID_34597310 ID_34529679 ID_32360945 ... Here, the output name on the list matches the source passed in to ft_links from ft_search. names(out) #&gt; [1] &quot;entrez&quot; You can alternatively pass in DOIs directly and specify the data source. Options include “plos”, “bmc”, “crossref”, and “entrez”. x &lt;- c(&quot;10.1371/journal.pone.0017342&quot;, &quot;10.1371/journal.pone.0091497&quot;) z &lt;- ft_links(x, from = &quot;plos&quot;) z$plos$data #&gt; $`10.1371/journal.pone.0017342` #&gt; $`10.1371/journal.pone.0017342`$xml #&gt; [1] &quot;http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0017342&amp;type=manuscript&quot; #&gt; #&gt; $`10.1371/journal.pone.0017342`$pdf #&gt; [1] &quot;http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0017342&amp;type=printable&quot; #&gt; #&gt; #&gt; $`10.1371/journal.pone.0091497` #&gt; $`10.1371/journal.pone.0091497`$xml #&gt; [1] &quot;http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0091497&amp;type=manuscript&quot; #&gt; #&gt; $`10.1371/journal.pone.0091497`$pdf #&gt; [1] &quot;http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0091497&amp;type=printable&quot; Fetch just the pdf links unname(vapply(z$plos$data, &quot;[[&quot;, &quot;&quot;, &quot;pdf&quot;)) #&gt; [1] &quot;http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0017342&amp;type=printable&quot; #&gt; [2] &quot;http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0091497&amp;type=printable&quot; 8.2 Links options All data sources for ft_links() SHOULD accept configuration options BUT that does not work right now. Fix coming, see https://github.com/ropensci/fulltext/issues/223 As all functions in fulltext, you can pass on curl options to each function call or set them globally for the session, see the curl options chapter. "],["fetch.html", "Chapter 9 Fetch 9.1 Data formats 9.2 How data is stored 9.3 Usage 9.4 Errors 9.5 Cleanup 9.6 Internals 9.7 Notes about specific data sources", " Chapter 9 Fetch The ft_get function makes it easy to fetch full text articles. There are a few different ways to use ft_get: Pass in only DOIs - leave from parameter NULL. This route will first query Crossref API for the publisher of the DOI, then we’ll use the appropriate method to fetch full text from the publisher. If a publisher is not found for the DOI, then we’ll throw back a message telling you a publisher was not found. Pass in DOIs (or other pub IDs) and use the from parameter. This route means we don’t have to make an extra API call to Crossref (thus, this route is faster) to determine the publisher for each DOI. We go straight to getting full text based on the publisher. Use ft_search() to search for articles. Then pass that output to this function, which will use info in that object. This behaves the same as the previous option in that each DOI has publisher info so we know how to get full text for each DOI. Note that some publishers are available through other data sources, e.g., through Entrez’s Pubmed. ft_get is a bit complicated. These are just some of the hurdles we’re jumping over: Negotiating various user inputs, likely seeing new publishers we’ve not dealt with Dealing with authentication and trying to make it easier for users Users sometimes being at an IP address that has access to a publisher and sometimes not Caching results to avoid unnecessary downloads if the content has already been acquired Thus, expect some hiccups here, and please do report problems, and if a certain publisher is not supported yet. 9.1 Data formats You can specify whether you want PDF, XML or plaint text with the type parameter. It is sometimes ignored, sometimes used, depending on the data source. For certain data sources, they only accept one type. Details by data source/publisher: PLOS: pdf and xml Entrez: only xml eLife: pdf and xml Pensoft: pdf and xml arXiv: only pdf BiorXiv: only pdf Elsevier: pdf and plain Wiley: only pdf Peerj: pdf and xml Informa: only pdf FrontiersIn: pdf and xml Copernicus: pdf and xml Scientific Societies: only pdf Crossref: depends on the publisher other data sources/publishers: there are too many to cover here - will try to make a helper in the future for what is covered by different publishers 9.2 How data is stored This depends on what backend value you use. If you use the default (rds) we store all data in .rds files. These are binary compressed files that are specific to R. Because they are specific to R, you don’t want to use this option if part of your downstream workflow is using another tool/programming language. The three types are stored in differnt ways. xml and plain text are parsed to plain text then stored with whatever backend you choose. However, pdf is retrived as raw bytes and stored as such. Thus, we no longer write pdf files to disk. However, you can easily do that yourself with ft_extract() or yourself by using pdftools::pdf_text which accepts a file path to a pdf or raw bytes. 9.3 Usage library(fulltext) List backends available ft_get_ls() #&gt; [1] &quot;aaas&quot; &quot;aip&quot; &quot;amersocclinoncol&quot; #&gt; [4] &quot;amersocmicrobiol&quot; &quot;arxiv&quot; &quot;biorxiv&quot; #&gt; [7] &quot;bmc&quot; &quot;cambridge&quot; &quot;cob&quot; #&gt; [10] &quot;copernicus&quot; &quot;crossref&quot; &quot;elife&quot; #&gt; [13] &quot;elsevier&quot; &quot;entrez&quot; &quot;frontiersin&quot; #&gt; [16] &quot;ieee&quot; &quot;informa&quot; &quot;instinvestfil&quot; #&gt; [19] &quot;jama&quot; &quot;microbiology&quot; &quot;peerj&quot; #&gt; [22] &quot;pensoft&quot; &quot;plos&quot; &quot;pnas&quot; #&gt; [25] &quot;royalsocchem&quot; &quot;roysoc&quot; &quot;sciencedirect&quot; #&gt; [28] &quot;scientificsocieties&quot; &quot;transtech&quot; &quot;wiley&quot; The simplest approach is passing a DOI directly to ft_get (res &lt;- ft_get(&#39;10.1371/journal.pone.0086169&#39;)) #&gt; &lt;fulltext text&gt; #&gt; [Docs] 1 #&gt; [Source] ext - /Users/sckott/Library/Caches/R/fulltext #&gt; [IDs] 10.1371/journal.pone.0086169 ... res$plos #&gt; $found #&gt; [1] 1 #&gt; #&gt; $dois #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $data #&gt; $data$backend #&gt; [1] &quot;ext&quot; #&gt; #&gt; $data$cache_path #&gt; [1] &quot;~/Library/Caches/R/fulltext&quot; #&gt; #&gt; $data$path #&gt; $data$path$`10.1371/journal.pone.0086169` #&gt; $data$path$`10.1371/journal.pone.0086169`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_1371_journal_pone_0086169.xml&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$id #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$error #&gt; NULL #&gt; #&gt; #&gt; #&gt; $data$data #&gt; NULL #&gt; #&gt; #&gt; $opts #&gt; $opts$doi #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $opts$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $opts$progress #&gt; [1] FALSE #&gt; #&gt; #&gt; $errors #&gt; id error #&gt; 1 10.1371/journal.pone.0086169 &lt;NA&gt; You can pass many DOIs in at once (res &lt;- ft_get(c(&#39;10.3389/fphar.2014.00109&#39;, &#39;10.3389/feart.2015.00009&#39;))) #&gt; &lt;fulltext text&gt; #&gt; [Docs] 2 #&gt; [Source] ext - /Users/sckott/Library/Caches/R/fulltext #&gt; [IDs] 10.3389/fphar.2014.00109 10.3389/feart.2015.00009 ... res$frontiersin #&gt; $found #&gt; [1] 2 #&gt; #&gt; $dois #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $data #&gt; $data$backend #&gt; [1] &quot;ext&quot; #&gt; #&gt; $data$cache_path #&gt; [1] &quot;~/Library/Caches/R/fulltext&quot; #&gt; #&gt; $data$path #&gt; $data$path$`10.3389/fphar.2014.00109` #&gt; $data$path$`10.3389/fphar.2014.00109`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_3389_fphar_2014_00109.xml&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$id #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$error #&gt; NULL #&gt; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009` #&gt; $data$path$`10.3389/feart.2015.00009`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_3389_feart_2015_00009.xml&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$id #&gt; [1] &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$error #&gt; NULL #&gt; #&gt; #&gt; #&gt; $data$data #&gt; NULL #&gt; #&gt; #&gt; $opts #&gt; $opts$dois #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $opts$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $opts$progress #&gt; [1] FALSE #&gt; #&gt; #&gt; $errors #&gt; id error #&gt; 1 10.3389/fphar.2014.00109 &lt;NA&gt; #&gt; 2 10.3389/feart.2015.00009 &lt;NA&gt; 9.4 Errors ft_get() for each article has an error slot. If the error slot is NULL then there is no error. If the error slot is not NULL there was an error, and the error message will be a character string in that slot. Possible errors include: An error reported by the web service e.g. “Timeout was reached: Connection timed out after 10003 milliseconds” No link found e.g. “no link found from Crossref” OR “has no link available” We attempted to fetch the article but the content type wasn’t what was expected. In this case we skip to the next article. e.g. “type was supposed to be pdf, but was text/html; charset=UTF-8” Weird uninformative errors e.g. “Recv failure: Operation timed out” OR “Operation was aborted by an application callback” An error associated mostly with PLOS. PLOS gives DOIs for parts of articles, like figures, so it doesn’t make sense to get full text of a figure. e.g., “was not found or may be a DOI for a part of an article” An additional top level slot called errors has a data.frame of all errors from each article, like: res &lt;- ft_get(c(&#39;10.7554/eLife.03032&#39;, &#39;10.7554/eLife.aaaa&#39;), from = &quot;elife&quot;) res$elife$errors id error 1 10.7554/eLife.03032 &lt;NA&gt; 2 10.7554/eLife.aaaa subscript out of bounds Where the second DOI was invalid. Granted the error in the data.frame “subscript out of bounds” isn’t very informative, but we can work on that. 9.5 Cleanup The above section about errors suggests that we often run into errors. When we run into errors downloading full text we capture the error message, if there is one, and delete the file we were trying to create. That is, we cleanup upon hitting an error such that you shouldn’t end up with blank files on your machine. Let us know if this isn’t true in your case and we’ll get it fixed. Note that even if you exit out of the all to ft_get() it should clean up a file if it is not completey done creating it, so you shouldn’t end up with bad files if you exit out of the function when it’s running. 9.6 Internals What’s going on under the hood in ft_get()? It goes like this: If you request data from a specific data source (use of from, only allowed for PLOS, Entrez, eLife, Pensoft, arXiv, BiorXiv, Elsevier and Wiley): Grab publisher specific collector function Each of these functions has specific code for that publisher to pull full text given an article identifier If you don’t request a specific data source: Guess which publisher the DOI comes from If publisher discovered that we have plugins for Get publisher specific collector function If publisher not discovered Ping the https://ftdoi.org API If publisher found with ftdoi API: Link for full text used from the ftdoi API response If publisher not found with ftdoi API: Attempt fetch via Crossref API - if links found we try those, some work and some don’t 9.7 Notes about specific data sources 9.7.1 Elsevier When you don’t have access to the full text of Elsevier articles they will often still give you something, but it will sometimes be just metadata of the paper or sometimes an abstract if you’re lucky. When you go to extract the text this will be rather obvious. "],["chunks.html", "Chapter 10 Extracting text 10.1 Usage 10.2 Tabularize 10.3 Other inputs", " Chapter 10 Extracting text Functions for extracting parts of texts used to live inside of fulltext, but have now moved to the package pubchunks. The pubchunks::pub_chunks function tries to make it easy to extract the parts of articles you want. This only works with XML format articles though since although we can get text out of PDFs, there is no machine readable way to say “I want the abstract”. In addition to only working with XML, this function only has knowledge about a select set of publishers for which we’ve encoded knowledge about how to get different sections of the article. Not all publishers use the same format XML - so each publisher is slightly different for how to get to each section. That is, to get to the abstract requires slightly different xpath for publisher A vs. publisher B vs. publisher C. An alternative to pubchunks is to use xpath or css selectors yourself to slice and dice XML. 10.1 Usage library(fulltext) library(pubchunks) Get a full text article x &lt;- ft_get(&#39;10.1371/journal.pone.0086169&#39;) Note that unlike previous versions of fulltext you now have to collect (ft_collect()) the text from the XML file on disk. Then you can pass to pub_chunks(), here to get authors. x %&gt;% ft_collect %&gt;% pub_chunks(&quot;authors&quot;) #&gt; $plos #&gt; $plos$`10.1371/journal.pone.0086169` #&gt; &lt;pub chunks&gt; #&gt; from: xml_document #&gt; publisher/journal: plos/PLoS ONE #&gt; sections: authors #&gt; showing up to first 5: #&gt; authors (n=4): nested list #&gt; #&gt; #&gt; attr(,&quot;ft_data&quot;) #&gt; [1] TRUE In another example, let’s search for PLOS articles. library(&quot;rplos&quot;) (dois &lt;- searchplos(q=&quot;*:*&quot;, fl=&#39;id&#39;, fq=list(&#39;doc_type:full&#39;,&quot;article_type:\\&quot;research article\\&quot;&quot;), limit=5)$data$id) #&gt; [1] &quot;10.1371/journal.pbio.1000153&quot; &quot;10.1371/journal.pbio.1000159&quot; #&gt; [3] &quot;10.1371/journal.pbio.1000167&quot; &quot;10.1371/journal.pbio.1000173&quot; #&gt; [5] &quot;10.1371/journal.pbio.1000176&quot; Then get the full text x &lt;- ft_get(dois) Then pull out various sections of each article. remember to pull out the full text first x &lt;- ft_collect(x) x %&gt;% pub_chunks(&quot;front&quot;) x %&gt;% pub_chunks(&quot;body&quot;) x %&gt;% pub_chunks(&quot;back&quot;) x %&gt;% pub_chunks(&quot;history&quot;) x %&gt;% pub_chunks(&quot;authors&quot;) x %&gt;% pub_chunks(c(&quot;doi&quot;,&quot;categories&quot;)) x %&gt;% pub_chunks(&quot;all&quot;) x %&gt;% pub_chunks(&quot;publisher&quot;) x %&gt;% pub_chunks(&quot;acknowledgments&quot;) x %&gt;% pub_chunks(&quot;permissions&quot;) x %&gt;% pub_chunks(&quot;journal_meta&quot;) x %&gt;% pub_chunks(&quot;article_meta&quot;) 10.2 Tabularize The function pub_tabularize() is useful for coercing the output of pub_chunks() into a data.frame, the lingua franca of data work in R. library(data.table) x &lt;- pub_chunks(x, c(&quot;doi&quot;, &quot;title&quot;)) x &lt;- pub_tabularize(x) rbindlist(x$plos, fill = TRUE) #&gt; doi #&gt; 1: 10.1371/journal.pbio.1000153 #&gt; 2: 10.1371/journal.pbio.1000159 #&gt; 3: 10.1371/journal.pbio.1000167 #&gt; 4: 10.1371/journal.pbio.1000173 #&gt; 5: 10.1371/journal.pbio.1000176 #&gt; title #&gt; 1: Emergence of a Stable Cortical Map for Neuroprosthetic Control #&gt; 2: Natural Killer Cell Signal Integration Balances Synapse Symmetry and Migration #&gt; 3: Ready…Go: Amplitude of the fMRI Signal Encodes Expectation of Cue Arrival Time #&gt; 4: Hippocampus Leads Ventral Striatum in Replay of Place-Reward Information #&gt; 5: β1 Integrin Maintains Integrity of the Embryonic Neocortical Stem Cell Niche #&gt; .publisher #&gt; 1: plos #&gt; 2: plos #&gt; 3: plos #&gt; 4: plos #&gt; 5: plos 10.3 Other inputs pub_chunks() works with other inputs besides the output of fulltext::ft_get(). 10.3.1 Files x &lt;- system.file(&quot;examples/10_1016_0021_8928_59_90156_x.xml&quot;, package = &quot;pubchunks&quot;) pub_chunks(x, &quot;abstract&quot;) #&gt; &lt;pub chunks&gt; #&gt; from: file #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: abstract #&gt; showing up to first 5: #&gt; abstract (n=1): Abstract #&gt; #&gt; This pa ... pub_chunks(x, &quot;title&quot;) #&gt; &lt;pub chunks&gt; #&gt; from: file #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: title #&gt; showing up to first 5: #&gt; title (n=1): On the driving of a piston with a rigid collar int ... pub_chunks(x, &quot;authors&quot;) #&gt; &lt;pub chunks&gt; #&gt; from: file #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: authors #&gt; showing up to first 5: #&gt; authors (n=1): Chetaev, D.N pub_chunks(x, c(&quot;title&quot;, &quot;refs&quot;)) #&gt; &lt;pub chunks&gt; #&gt; from: file #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: title, refs #&gt; showing up to first 5: #&gt; title (n=1): On the driving of a piston with a rigid collar int ... #&gt; refs (n=6): Watson G.N.. 1949. Teoriia besselevykh funktsii. N The output of pub_chunks() is a list with an S3 class pub_chunks to make internal work in the package easier. You can easily see the list structure by using unclass(). 10.3.2 xml in a string xml &lt;- paste0(readLines(x), collapse = &quot;&quot;) pub_chunks(xml, &quot;title&quot;) #&gt; &lt;pub chunks&gt; #&gt; from: character #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: title #&gt; showing up to first 5: #&gt; title (n=1): On the driving of a piston with a rigid collar int ... 10.3.3 xml2 objects xml &lt;- paste0(readLines(x), collapse = &quot;&quot;) xml &lt;- xml2::read_xml(xml) pub_chunks(xml, &quot;title&quot;) #&gt; &lt;pub chunks&gt; #&gt; from: xml_document #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: title #&gt; showing up to first 5: #&gt; title (n=1): On the driving of a piston with a rigid collar int ... "],["on-disk.html", "Chapter 11 Summarize articles on disk 11.1 Usage", " Chapter 11 Summarize articles on disk The ft_table() function makes it easy to create a data.frame of the text of PDF, plain text, and XML files, together with DOIs/IDs for each article. It’s similar to the readtext::readtext() function, but is much more specific to just this package. With the output of ft_table() you can go directly into a text-mining package like quanteda. 11.1 Usage library(fulltext) Use ft_table() to pull out text from all articles. ft_table() #&gt; # A tibble: 9 × 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1371/journal.pbio.1000153 10_1371_journal_pbio_1000153 &quot;plosPLoS… /Users/s… #&gt; 2 10.1371/journal.pbio.1000159 10_1371_journal_pbio_1000159 &quot;plosPLoS… /Users/s… #&gt; 3 10.1371/journal.pbio.1000167 10_1371_journal_pbio_1000167 &quot;plosPLoS… /Users/s… #&gt; 4 10.1371/journal.pbio.1000173 10_1371_journal_pbio_1000173 &quot;plosPLoS… /Users/s… #&gt; 5 10.1371/journal.pbio.1000176 10_1371_journal_pbio_1000176 &quot;plosPLoS… /Users/s… #&gt; 6 10.1371/journal.pone.0086169 10_1371_journal_pone_0086169 &quot;PLoS ONE… /Users/s… #&gt; 7 10.3389/feart.2015.00009 10_3389_feart_2015_00009 &quot;Front. E… /Users/s… #&gt; 8 10.3389/fphar.2014.00109 10_3389_fphar_2014_00109 &quot;Front. P… /Users/s… #&gt; 9 3399982 3399982 &quot;03727252… /Users/s… You can pull out just text from XML files ft_table(type = &quot;xml&quot;) #&gt; # A tibble: 9 × 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1371/journal.pbio.1000153 10_1371_journal_pbio_1000153 &quot;plosPLoS… /Users/s… #&gt; 2 10.1371/journal.pbio.1000159 10_1371_journal_pbio_1000159 &quot;plosPLoS… /Users/s… #&gt; 3 10.1371/journal.pbio.1000167 10_1371_journal_pbio_1000167 &quot;plosPLoS… /Users/s… #&gt; 4 10.1371/journal.pbio.1000173 10_1371_journal_pbio_1000173 &quot;plosPLoS… /Users/s… #&gt; 5 10.1371/journal.pbio.1000176 10_1371_journal_pbio_1000176 &quot;plosPLoS… /Users/s… #&gt; 6 10.1371/journal.pone.0086169 10_1371_journal_pone_0086169 &quot;PLoS ONE… /Users/s… #&gt; 7 10.3389/feart.2015.00009 10_3389_feart_2015_00009 &quot;Front. E… /Users/s… #&gt; 8 10.3389/fphar.2014.00109 10_3389_fphar_2014_00109 &quot;Front. P… /Users/s… #&gt; 9 3399982 3399982 &quot;03727252… /Users/s… You can pull out just text from PDF files ft_table(type = &quot;pdf&quot;) #&gt; # A tibble: 0 × 3 #&gt; # … with 3 variables: ids_norm &lt;chr&gt;, text &lt;chr&gt;, paths &lt;chr&gt; You can pull out XML but not extract the text. So you’ll get XML strings that you can parse yourself with xpath/css selectors/etc. ft_table(xml_extract_text = FALSE) #&gt; # A tibble: 9 × 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1371/journal.pbio.1000153 10_1371_journal_pbio_1000153 &quot;&lt;?xml ve… /Users/s… #&gt; 2 10.1371/journal.pbio.1000159 10_1371_journal_pbio_1000159 &quot;&lt;?xml ve… /Users/s… #&gt; 3 10.1371/journal.pbio.1000167 10_1371_journal_pbio_1000167 &quot;&lt;?xml ve… /Users/s… #&gt; 4 10.1371/journal.pbio.1000173 10_1371_journal_pbio_1000173 &quot;&lt;?xml ve… /Users/s… #&gt; 5 10.1371/journal.pbio.1000176 10_1371_journal_pbio_1000176 &quot;&lt;?xml ve… /Users/s… #&gt; 6 10.1371/journal.pone.0086169 10_1371_journal_pone_0086169 &quot;&lt;?xml ve… /Users/s… #&gt; 7 10.3389/feart.2015.00009 10_3389_feart_2015_00009 &quot;&lt;?xml ve… /Users/s… #&gt; 8 10.3389/fphar.2014.00109 10_3389_fphar_2014_00109 &quot;&lt;?xml ve… /Users/s… #&gt; 9 3399982 3399982 &quot;&lt;?xml ve… /Users/s… "],["curl-options.html", "Chapter 12 Request debugging/inspection", " Chapter 12 Request debugging/inspection xxxxxx "],["use-cases.html", "Chapter 13 Use cases", " Chapter 13 Use cases Coming soon … "],["session-info.html", "Chapter 14 session info", " Chapter 14 session info Session info for this book sessioninfo::session_info() #&gt; ─ Session info ─────────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 4.1.1 (2021-08-10) #&gt; os macOS Big Sur 11.6 #&gt; system aarch64, darwin20.5.0 #&gt; ui unknown #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz America/Los_Angeles #&gt; date 2021-10-04 #&gt; #&gt; ─ Packages ─────────────────────────────────────────────────────────────────── #&gt; package * version date lib source #&gt; aRxiv 0.5.19 2019-08-08 [1] CRAN (R 4.1.1) #&gt; bookdown 0.24 2021-09-02 [1] CRAN (R 4.1.1) #&gt; bslib 0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0) #&gt; cli 3.0.1 2021-07-17 [1] CRAN (R 4.1.0) #&gt; colorspace 2.0-2 2021-06-24 [1] CRAN (R 4.1.0) #&gt; crayon 1.4.1 2021-02-08 [1] CRAN (R 4.1.0) #&gt; crul 1.1.0.93 2021-08-16 [1] local #&gt; curl 4.3.2 2021-06-23 [1] CRAN (R 4.1.0) #&gt; data.table * 1.14.0 2021-02-21 [1] CRAN (R 4.1.0) #&gt; digest 0.6.27 2020-10-24 [1] CRAN (R 4.1.0) #&gt; dplyr 1.0.7 2021-06-18 [1] CRAN (R 4.1.0) #&gt; DT 0.18 2021-04-14 [1] CRAN (R 4.1.0) #&gt; ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.1.0) #&gt; evaluate 0.14 2019-05-28 [1] CRAN (R 4.1.0) #&gt; fansi 0.5.0 2021-05-25 [1] CRAN (R 4.1.0) #&gt; fastmap 1.1.0 2021-01-25 [1] CRAN (R 4.1.0) #&gt; fulltext * 2.0 2021-10-04 [1] Github (ropensci/fulltext@3bdfb86) #&gt; generics 0.1.0 2020-10-31 [1] CRAN (R 4.1.0) #&gt; ggplot2 3.3.5 2021-06-25 [1] CRAN (R 4.1.0) #&gt; glue 1.4.2 2020-08-27 [1] CRAN (R 4.1.0) #&gt; gtable 0.3.0 2019-03-25 [1] CRAN (R 4.1.0) #&gt; hoardr 0.5.2 2018-12-02 [1] CRAN (R 4.1.0) #&gt; htmltools 0.5.2 2021-08-25 [1] CRAN (R 4.1.1) #&gt; htmlwidgets 1.5.3 2020-12-10 [1] CRAN (R 4.1.0) #&gt; httpcode 0.3.0 2020-04-10 [1] CRAN (R 4.1.0) #&gt; httpuv 1.6.2 2021-08-18 [1] CRAN (R 4.1.1) #&gt; httr 1.4.2 2020-07-20 [1] CRAN (R 4.1.0) #&gt; jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.1.0) #&gt; jsonlite 1.7.2 2020-12-09 [1] CRAN (R 4.1.0) #&gt; knitr 1.33 2021-04-24 [1] CRAN (R 4.1.0) #&gt; later 1.3.0 2021-08-18 [1] CRAN (R 4.1.1) #&gt; lifecycle 1.0.0 2021-02-15 [1] CRAN (R 4.1.0) #&gt; lubridate 1.7.10 2021-02-26 [1] CRAN (R 4.1.1) #&gt; magrittr 2.0.1 2020-11-17 [1] CRAN (R 4.1.0) #&gt; microdemic 0.6.0 2020-11-20 [1] CRAN (R 4.1.1) #&gt; mime 0.11 2021-06-23 [1] CRAN (R 4.1.0) #&gt; miniUI 0.1.1.1 2018-05-18 [1] CRAN (R 4.1.0) #&gt; munsell 0.5.0 2018-06-12 [1] CRAN (R 4.1.0) #&gt; pillar 1.6.2 2021-07-29 [1] CRAN (R 4.1.0) #&gt; pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.1.0) #&gt; plyr 1.8.6 2020-03-03 [1] CRAN (R 4.1.0) #&gt; promises 1.2.0.1 2021-02-11 [1] CRAN (R 4.1.0) #&gt; pubchunks * 0.4.0 2021-09-10 [1] local #&gt; purrr 0.3.4 2020-04-17 [1] CRAN (R 4.1.0) #&gt; R6 2.5.1 2021-08-19 [1] CRAN (R 4.1.1) #&gt; rappdirs 0.3.3 2021-01-31 [1] CRAN (R 4.1.0) #&gt; Rcpp 1.0.7 2021-07-07 [1] CRAN (R 4.1.0) #&gt; rcrossref 1.1.0.99 2021-10-04 [1] Github (ropensci/rcrossref@319f34c) #&gt; rentrez 1.2.3 2020-11-10 [1] CRAN (R 4.1.0) #&gt; reshape2 1.4.4 2020-04-09 [1] CRAN (R 4.1.1) #&gt; rlang 0.4.11 2021-04-30 [1] CRAN (R 4.1.0) #&gt; rmarkdown 2.10 2021-08-06 [1] CRAN (R 4.1.1) #&gt; rplos * 1.0.0 2021-02-23 [1] CRAN (R 4.1.1) #&gt; rstudioapi 0.13 2020-11-12 [1] CRAN (R 4.1.0) #&gt; sass 0.4.0 2021-05-12 [1] CRAN (R 4.1.0) #&gt; scales 1.1.1 2020-05-11 [1] CRAN (R 4.1.0) #&gt; sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 4.1.0) #&gt; shiny 1.6.0 2021-01-25 [1] CRAN (R 4.1.0) #&gt; solrium 1.2.0 2021-05-19 [1] CRAN (R 4.1.0) #&gt; storr 1.2.5 2020-12-01 [1] CRAN (R 4.1.1) #&gt; stringi 1.7.4 2021-08-25 [1] CRAN (R 4.1.1) #&gt; stringr 1.4.0 2019-02-10 [1] CRAN (R 4.1.0) #&gt; tibble 3.1.4 2021-08-25 [1] CRAN (R 4.1.1) #&gt; tidyselect 1.1.1 2021-04-30 [1] CRAN (R 4.1.0) #&gt; triebeard 0.3.0 2016-08-04 [1] CRAN (R 4.1.0) #&gt; urltools 1.7.3 2019-04-14 [1] CRAN (R 4.1.0) #&gt; utf8 1.2.2 2021-07-24 [1] CRAN (R 4.1.0) #&gt; vctrs 0.3.8 2021-04-29 [1] CRAN (R 4.1.0) #&gt; whisker 0.4 2019-08-28 [1] CRAN (R 4.1.0) #&gt; withr 2.4.2 2021-04-18 [1] CRAN (R 4.1.0) #&gt; xfun 0.25 2021-08-06 [1] CRAN (R 4.1.1) #&gt; XML 3.99-0.7 2021-08-17 [1] CRAN (R 4.1.1) #&gt; xml2 1.3.2 2020-04-23 [1] CRAN (R 4.1.0) #&gt; xtable 1.8-4 2019-04-21 [1] CRAN (R 4.1.0) #&gt; yaml 2.2.1 2020-02-01 [1] CRAN (R 4.1.0) #&gt; #&gt; [1] /opt/homebrew/lib/R/4.1/site-library #&gt; [2] /opt/homebrew/Cellar/r/4.1.1/lib/R/library "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
